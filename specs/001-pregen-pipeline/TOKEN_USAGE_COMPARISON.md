# Token Usage Comparison: Full vs Lean Models

## Visual Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FULL LEARNINGITEM MODEL                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM must generate 17 fields:                                â”‚
â”‚                                                              â”‚
â”‚ âœ“ target_item: str                  [NEEDED FROM LLM]       â”‚
â”‚ âœ“ definition: str                   [NEEDED FROM LLM]       â”‚
â”‚ âœ“ examples: List[Example]           [NEEDED FROM LLM]       â”‚
â”‚                                                              â”‚
â”‚ âœ— id: str                           [Generated: uuid4()]    â”‚
â”‚ âœ— language: str                     [Known: "zh"]           â”‚
â”‚ âœ— category: Category                [Known: PRONUNCIATION]  â”‚
â”‚ âœ— romanization: Optional[str]       [Auto: pypinyin]        â”‚
â”‚ âœ— sense_gloss: Optional[str]        [Optional]              â”‚
â”‚ âœ— lemma: Optional[str]              [Optional]              â”‚
â”‚ âœ— pos: Optional[str]                [Optional]              â”‚
â”‚ âœ— aliases: List[str]                [Auto-generated]        â”‚
â”‚ âœ— media_urls: List[str]             [Usually empty]         â”‚
â”‚ âœ— level_system: LevelSystem         [Known: HSK]            â”‚
â”‚ âœ— level_min: str                    [Known: "HSK1"]         â”‚
â”‚ âœ— level_max: str                    [Known: "HSK1"]         â”‚
â”‚ âœ— created_at: datetime              [Generated: now()]      â”‚
â”‚ âœ— version: str                      [Constant: "1.0.0"]     â”‚
â”‚ âœ— source_file: Optional[str]        [Optional]              â”‚
â”‚                                                              â”‚
â”‚ RESULT: 3 needed, 14 wasted âŒ                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            â¬‡ï¸  OPTIMIZATION  â¬‡ï¸

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEAN LEARNINGITEM MODEL                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM generates only 3 essential fields:                      â”‚
â”‚                                                              â”‚
â”‚ âœ“ target_item: str                  [NEEDED FROM LLM]       â”‚
â”‚ âœ“ definition: str                   [NEEDED FROM LLM]       â”‚
â”‚ âœ“ examples: List[str]               [NEEDED FROM LLM]       â”‚
â”‚                                                              â”‚
â”‚ Post-processing adds metadata via _assemble_learning_items()â”‚
â”‚                                                              â”‚
â”‚ RESULT: 3 needed, 0 wasted âœ…                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Token Breakdown

### System Prompt (Schema Description)

```
Full Model Schema:        Lean Model Schema:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚     â”‚                  â”‚
â”‚  350 tokens      â”‚     â”‚   60 tokens      â”‚
â”‚                  â”‚     â”‚                  â”‚
â”‚ â€¢ 17 fields      â”‚     â”‚ â€¢ 3 fields       â”‚
â”‚ â€¢ Nested schemas â”‚     â”‚ â€¢ Simple types   â”‚
â”‚ â€¢ Enums          â”‚     â”‚ â€¢ No nesting     â”‚
â”‚ â€¢ Descriptions   â”‚     â”‚ â€¢ Brief desc     â”‚
â”‚                  â”‚     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â¬‡ï¸                        â¬‡ï¸
   PER CALL                   PER CALL
      
Savings: 290 tokens (83% reduction) âœ¨
```

### LLM Response (Per Item)

```
Full Model Response:           Lean Model Response:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ {                       â”‚   â”‚ {                       â”‚
â”‚   "id": "abc123...",    â”‚   â”‚   "target_item": "...", â”‚
â”‚   "language": "zh",     â”‚   â”‚   "definition": "...",  â”‚
â”‚   "category": "...",    â”‚   â”‚   "examples": [         â”‚
â”‚   "target_item": "...", â”‚   â”‚     "ä¾‹å¥1",            â”‚
â”‚   "definition": "...",  â”‚   â”‚     "ä¾‹å¥2",            â”‚
â”‚   "examples": [         â”‚   â”‚     "ä¾‹å¥3"             â”‚
â”‚     {                   â”‚   â”‚   ]                     â”‚
â”‚       "text": "...",    â”‚   â”‚ }                       â”‚
â”‚       "translation": "" â”‚   â”‚                         â”‚
â”‚       "media_urls": []  â”‚   â”‚  80 tokens              â”‚
â”‚     },                  â”‚   â”‚                         â”‚
â”‚     ...                 â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   ],                    â”‚
â”‚   "romanization": "",   â”‚
â”‚   "sense_gloss": null,  â”‚
â”‚   "lemma": null,        â”‚
â”‚   "pos": null,          â”‚
â”‚   "aliases": [],        â”‚
â”‚   "media_urls": [],     â”‚
â”‚   "level_system": "hsk",â”‚
â”‚   "level_min": "HSK1",  â”‚
â”‚   "level_max": "HSK1",  â”‚
â”‚   "created_at": "...",  â”‚
â”‚   "version": "1.0.0",   â”‚
â”‚   "source_file": null   â”‚
â”‚ }                       â”‚
â”‚                         â”‚
â”‚  200 tokens             â”‚
â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Savings: 120 tokens per item (60% reduction) âœ¨
```

## Batch Generation Example

**Scenario:** Generate 10 pronunciation items for Chinese HSK1

### Full Model Approach (Before)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Call #1: Pronunciation Items                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚ System Prompt:         350 tokens                   â”‚
â”‚ User Prompt:           500 tokens                   â”‚
â”‚ LLM Response:        2,000 tokens (10 items Ã— 200)  â”‚
â”‚                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚ TOTAL:              2,850 tokens                    â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Lean Model Approach (After)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Call #1: Pronunciation Items                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚ System Prompt:          60 tokens  âœ… (-290)        â”‚
â”‚ User Prompt:           500 tokens  (unchanged)      â”‚
â”‚ LLM Response:          800 tokens  âœ… (-1,200)      â”‚
â”‚                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚ TOTAL:              1,360 tokens                    â”‚
â”‚                                                      â”‚
â”‚ SAVINGS:            1,490 tokens (52%) ğŸ‰           â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Complete Pipeline (8 Categories)

### Full Model Approach

```
Category              â”‚ Tokens  â”‚ Cost (Sonnet)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pronunciation         â”‚  2,850  â”‚  $0.043
Idiom                 â”‚  2,850  â”‚  $0.043
Functional            â”‚  2,850  â”‚  $0.043
Cultural              â”‚  2,850  â”‚  $0.043
Writing System        â”‚  2,850  â”‚  $0.043
Sociolinguistic       â”‚  2,850  â”‚  $0.043
Pragmatic             â”‚  2,850  â”‚  $0.043
Literacy              â”‚  2,850  â”‚  $0.043
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                 â”‚ 22,800  â”‚  $0.342
```

### Lean Model Approach

```
Category              â”‚ Tokens  â”‚ Cost (Sonnet)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pronunciation         â”‚  1,360  â”‚  $0.020
Idiom                 â”‚  1,360  â”‚  $0.020
Functional            â”‚  1,360  â”‚  $0.020
Cultural              â”‚  1,360  â”‚  $0.020
Writing System        â”‚  1,360  â”‚  $0.020
Sociolinguistic       â”‚  1,360  â”‚  $0.020
Pragmatic             â”‚  1,360  â”‚  $0.020
Literacy              â”‚  1,360  â”‚  $0.020
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL                 â”‚ 10,880  â”‚  $0.163
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAVINGS               â”‚ 11,920  â”‚  $0.179 âœ¨
                      â”‚  (52%)  â”‚  (52%)
```

## Cost Comparison at Scale

### Per Language/Level

```
Full Model:  $0.342
Lean Model:  $0.163
           â”€â”€â”€â”€â”€â”€
SAVINGS:     $0.179 per generation
```

### For 300 Generations (50 languages Ã— 6 levels)

```
Full Model:  $0.342 Ã— 300 = $102.60
Lean Model:  $0.163 Ã— 300 =  $48.90
                           â”€â”€â”€â”€â”€â”€â”€â”€
SAVINGS:                     $53.70 âœ¨
```

### Combined with Batch Optimization

**Batch optimization alone:**
- Loop approach: 523 API calls
- Batch approach: 8 API calls
- Token savings: ~257,500 (system prompts)
- Cost savings: ~$78 per 300 runs

**Batch + Lean models:**
- API calls: 523 â†’ 8 (65Ã— reduction)
- Token savings: ~269,420 (system + responses)
- Cost savings: ~$131.70 per 300 runs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         COMBINED OPTIMIZATION IMPACT                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚ Original (Loop + Full Model):                       â”‚
â”‚   â€¢ API Calls:     523 per language/level           â”‚
â”‚   â€¢ Tokens:       ~350,000 per language/level       â”‚
â”‚   â€¢ Time:          17.4 minutes                     â”‚
â”‚   â€¢ Cost:          $0.520 per language/level        â”‚
â”‚                                                      â”‚
â”‚ Optimized (Batch + Lean):                           â”‚
â”‚   â€¢ API Calls:       8 per language/level           â”‚
â”‚   â€¢ Tokens:       ~90,000 per language/level        â”‚
â”‚   â€¢ Time:            24 seconds                     â”‚
â”‚   â€¢ Cost:          $0.163 per language/level        â”‚
â”‚                                                      â”‚
â”‚ IMPROVEMENTS:                                        â”‚
â”‚   â€¢ API Calls:    65Ã— fewer                         â”‚
â”‚   â€¢ Tokens:       74% reduction                     â”‚
â”‚   â€¢ Time:         43Ã— faster                        â”‚
â”‚   â€¢ Cost:         69% cheaper                       â”‚
â”‚                                                      â”‚
â”‚ FOR 300 RUNS:                                        â”‚
â”‚   â€¢ Original:     $156.00                           â”‚
â”‚   â€¢ Optimized:     $48.90                           â”‚
â”‚   â€¢ SAVED:        $107.10 ğŸ‰                        â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Code Example Comparison

### Before: Full Model with Metadata Assignment

```python
# Request full LearningItem from LLM
response = self.llm_client.generate(
    prompt=user_prompt,
    response_model=LearningItemBatch,  # 17 fields
    system_prompt=system_prompt,
)

# Manually set metadata for each item (redundant)
for item in response.items:
    item.language = self.language              # âŒ LLM just generated this
    item.category = Category.PRONUNCIATION     # âŒ LLM just generated this
    item.level_system = self.level_system      # âŒ LLM just generated this
    item.level_min = self.level                # âŒ LLM just generated this
    item.level_max = self.level                # âŒ LLM just generated this

return response.items
```

### After: Lean Model with Assembly

```python
# Request only essential fields from LLM
response = self.llm_client.generate(
    prompt=user_prompt,
    response_model=LeanLearningItemBatch,  # 3 fields only
    system_prompt=system_prompt,
)

# Assemble full objects with known metadata
return self._assemble_learning_items(
    response.items,
    Category.PRONUNCIATION
)  # âœ… Metadata added once in assembly
```

## Summary

| Aspect | Full Model | Lean Model | Improvement |
|--------|-----------|------------|-------------|
| **Schema tokens** | 350 | 60 | **83% smaller** |
| **Response tokens/item** | 200 | 80 | **60% smaller** |
| **Total tokens/call** | 2,850 | 1,360 | **52% fewer** |
| **Cost/call** | $0.043 | $0.020 | **53% cheaper** |
| **Cost/300 runs** | $102.60 | $48.90 | **$53.70 saved** |
| **Maintainability** | Scattered metadata | Centralized assembly | **Better** |
| **Consistency** | Different from enrichers | Matches enrichers | **Better** |

---

**Key Insight:** The LLM should only generate **content** (what to learn, how to explain it, examples). Everything elseâ€”metadata, IDs, timestampsâ€”should be handled by code. This is the same pattern used successfully in vocab/grammar enrichers. ğŸ¯
